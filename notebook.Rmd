---
title: "Data Import and Rudamentary Analysis"
output: html_notebook
---

```{r, setup}
library(tidyverse)
library(e1071) # for SVMs
library(foreach) # also for bagging
library(randomForest)

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.
```{r, import}
subtypes <- read_tsv('subtypes.txt', 
                     col_names=TRUE,
                     col_types = list(
                       cellline = col_character(),
                       subtype = col_character())
                     )
# subtypes_binary <- subset(transform(subtypes, 
#                                     'normal_like' = as.factor(subtype == "Normal-like"),
#                                     'luminal' = as.factor(subtype == "Luminal"),
#                                     'claudin_low' = as.factor(subtype == "Claudin-low"),
#                                     'basal' = as.factor(subtype == "Basal")),
#                           select=-c(subtype))

subtypes$subtype <- as.factor(subtypes$subtype)

scoring <- read_csv('scoring_and_test_set_id_mappings.csv', 
                    col_names = TRUE,
                    col_types = list(
                      cellline = col_character(), 
                      drug = col_character(),
                      id = col_integer(),
                      Usage = col_character()))

# discovered drug naming mismatch, making them the same this way
scoring$drug <- gsub("-", ".", scoring$drug)

sample_submission <- read_csv('rand_sub_cont.csv',
                              col_names = TRUE,
                              col_types = list(
                                id = col_integer(),
                                value = col_double()))

expression <- read.table('expression.txt', 
                         header=TRUE, 
                         sep = "\t", 
                         row.names=1,
                         quote = "",
                         check.names=FALSE)


training_set_results <- read.table('training_set_answers.txt',
                                   header=TRUE,
                                   sep = '\t',
                                   row.names=1,
                                   quote = "",
                                   colClasses="factor")


expression = as_tibble(rownames_to_column(as.data.frame(t(expression)), var='cellline'))
training_set_results = as_tibble(rownames_to_column(training_set_results, var='cellline'))
```


# EDA

First, we look for missing values, as their precense will make a large impact on how we proceed

```{r missing}
anyNA(expression)
anyNA(training_set_results)
anyNA(subtypes)
anyNA(scoring)
```

Now that we can see that we have no missing data, we can now evaluate types.  

```{r typeof_subtypes}
sapply(subtypes, class)
```

```{r typeof_expression}
table(sapply(expression, class))
class(expression$cellline)
```

```{r type_of_training_set_results}
table(sapply(training_set_results, class))
class(training_set_results$cellline)
```

Now, we are happy that we have no missing data we have to address, and all of the data we imported is stored in the correct type for the purposes of analysis in `R`, we now go on to check for normalization in the gene expression data.  To do that, I evaluate the quantiles for each cell line, since I'm not actually interested in what the actual values, I'm interested in how much they vary.

```{r, normalization_verify_1}
expression_quantiles <- apply(expression[,-1], 1, quantile, probs=c(.25,.5, .75))
apply(expression_quantiles, 1, var)
```

If we're not happy with just checking the quantiles alone, we can look at a row-wise boxplot.

```{r, normalization_verify_2}
boxplot(t(subset(expression, select=-c(cellline))), 
        names=expression$cellline,
        # xlab='Cell-line',
        las=2)
title("Comparing Gene Expression Values")
```

From the results, I can see the quantiles are all similar, and am satisified that the expression data does not require any further normalization.


# Classification

The next step in our analysis is to slim down our feature space.  We are working with `r length(expression)` genes, which is not really feasible to try and make work.  The first thing we do is remove the genes from the analysis that have low variance, or in other words, genes that seem to not change regardless of which sample.  To determine the cutoff, we analyze what the gene variance looks like.

Since we expect an exponential decay of variances, we look at the log-scale of the variances to see if there is an obvious cut-off.

```{r, variances}
par(mfrow=c(1,2))
variances <- apply(expression[,-1], 2, var)


histogram.data <- hist(variances, breaks=30, plot=TRUE)
histogram.data$counts <- log(histogram.data$counts, 10)
plot(histogram.data, ylim=c(0, 5), ylab=expression("log"[10]*" Frequency"))

```

While we can see that there is a change in how the histogram showing what the variance is like when we approach variance values of 5, we will cloose a much lower threshold due to the remainder of our analysis plan, with which the first step involves determining the eigen values and eigen vectors from the co-variance matrix for the purposes of doing a PCA.  Numerical solvers are fairly good, however calculating the covariance between every gene involves making $n^2$ co-variance calculations.  Since we're starting with `r length(expression)` genes, that would result in  `r length(expression)`$^2$ variance calculations (where each feature has 39 features to it), so we need to slim it down somewhat.


```{r, pca_1}
cutoff = 2
features_of_interest <- colnames(expression[,-1])[which(variances > cutoff)]
expressions_pruned <- expression[,features_of_interest]
```

At this point we have `r length(features_of_interest)` features, and given that we have `r ncol(expression)` observations, this feature space is still far too large to do analysis on its own, which is why we now perform a PCA where we look at doing dimension reduction.  When performing a PCA, we take the eigen values the covariance matrix.  Each eigen value corresponds to the error that would be introduced should we exclude the corresponding eigen vector and lessen the dimension by 1. 


```{r, eigen_calcs}
ev <- eigen(cov(expressions_pruned))
```

To determine how much of a dimension reduction we can do without introducing too much error, we plot the eigen-values.

```{r, pca_error_plot}
par(mfrow=c(1,2))
error.data<- plot(ev$values, 
                  type="l",
                  xlab="Eigen Value Index",
                  ylab="Error")
plot(log10(ev$values), 
     type="l",
     xlab="Eigen Value Index",
     ylab=expression("log"[10]*" Error"))
```
Again, the first plot does not tell us much, other than the error quickly goes to near 0, suggesting that we can likely do some major feature reduction; however the log-scale plot also shows that there is a rapid transition from a high degree of error, (on the order of $10^0$) to a very small error ($10^{-15}$).  

```{r pca_selection}
expressions_pca <- as.data.frame(as.matrix(expressions_pruned) %*% 
                                   ev$vectors[,1:length(ev$values[which(ev$values > .1)])])
```


We ended up going down to `r ncol(expressions_pca)` dimensions, and given our sample size that's far more managable.  At this point, I am now ready to begin my bagging/bootstrapping.

```{r combining_training_data}
training_data <- inner_join(training_set_results,
                            bind_cols(expressions_pca, subtypes),
                            by = "cellline")

training_data <- gather(training_data, "drug", "response", 2:13)
training_data$response <- as.factor(training_data$response)

test_data <- inner_join(tibble('cellline' = setdiff(expression$cellline, 
                                training_set_results$cellline)),     
                        bind_cols(expressions_pca, subtypes), 
                        by = 'cellline')
```


Now that we have the test and training data in place, as well as our feature reduction, we can move onto our modeling methods.

# Linear Regression

Since we need to make two submissions, the first submission will make use of linear regression, to make linear regression work with our obscenely small sample size, we need to trim down the feature space significantly. Even though we got down to `r ncol(expressions_pca)` features, since we only have `r nrow(training_data)` samples, we won't be able to make use of all those features.  As a result, I fully expect the linear regression model to behave poorly, but we are treating this submission as an exercise in developing the method from first principles.

First we create our bagging function, which given n observations, it will make n samples with replacements as many times as we want (specified a default of 500 times), calculate the prediction parameters and then average them out.

```{r, linear_regression and bagging}
bagging <- function(training, test_vector, drug, iterations=500)
{
  training_data_slim <- subset(filter(training, 
                                      drug == drug), 
                               select=-c(drug, cellline))
  predictions <- foreach(m=1:iterations, .combine=cbind)  %do% {
    training_bag_set <- sample_n(training_data_slim, 
                                 size=nrow(training_data_slim), 
                                 replace=TRUE)
    suppressWarnings(fit <- lm(as.numeric(levels(response))[response] ~ ., 
                               data=training_bag_set))
    suppressWarnings(predict(fit, newdata=test_data))
  }
  rowMeans(predictions)
}

outcome <- tibble('cellline' = scoring$cellline,
                  'drug' = scoring$drug,
                  'id' = scoring$id,
                  'value' = NA)

for(drug in unique(training_data$drug)){
  predictions <- bagging(training_data, test_data, drug)
  for(j in seq_along(test_data$cellline)){
    cellline <- test_data$cellline[j]
    row <- which(outcome$cellline == cellline & outcome$drug == drug)
    outcome$value[row] <- predictions[j]
  }
}

write_csv(outcome[,3:4], 'submission_linear_regression.csv')
```

Amazingly enough, the linear regression model didn't do poorly.  At the time of submission, it was not the worst submssion made (but second worst), and resulted in a score of 0.51.  Some notes, due to the extreme amount of error I was introducing by eliminating parameters from the eigen vectors due to my small sample size, we decided to exclude the use of the subtype variable.  It is also possible that accuracy will improve if I was to inrease the iterations, but at 500 iterations of 25 choose 25 with replacement, it's most likely we are converging.

# Random Forests

The next model type we elected to use was random forests.  Random forests lends itself really nicely to us, as it has no problem handling highly dimension data with very small sample sizes.  Even though through the PCA, we reduced the expression set data down to 38 dimensions, that's still more than the number of training samples we have to work with.  

```{r random_forest}
models <- training_data %>%
            select(-cellline) %>%
            group_by(drug) %>%
            nest() %>%
            mutate(model = map(data, ~ randomForest(response ~ ., 
                                                    data=., 
                                                    importance=TRUE, 
                                                    ntree=200000)))


outcome <- tibble('cellline' = scoring$cellline,
                  'drug' = scoring$drug,
                  'id' = scoring$id,
                  'value' = NA)

for(i in seq_along(models$drug)){
  drug <- models$drug[i]
  predictions <- as.vector(predict(models$model[[i]], test_data[,-1]))
  for(j in seq_along(test_data$cellline)){
    cellline <- test_data$cellline[j]
    row <- which(outcome$cellline == cellline & outcome$drug == drug)
    outcome$value[row] <- predictions[j]
  }
}

write_csv(outcome[,3:4], 'submission_random_forests.csv')
```

The random forests model generates significantly better results than the linear regression model, giving us a score of 0.64.  Given the simplicity of the algorithm, for a safe and fast analysis, random forests seem to be an excellent way to go.

