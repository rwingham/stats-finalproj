---
title: "Data Import and Rudamentary Analysis"
output: html_notebook
---

```{r, setup}
library(tidyverse)

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.
```{r, import}
subtypes <- read_tsv('subtypes.txt', 
                     col_names=TRUE,
                     col_types = list(
                       cellline = col_character(),
                       subtype = col_character())
                     )

scoring <- read_csv('scoring_and_test_set_id_mappings.csv', 
                    col_names = TRUE,
                    col_types = list(
                      cellline = col_character(), 
                      drug = col_character(),
                      id = col_integer(),
                      Usage = col_character()))

sample_submission <- read_csv('rand_sub_cont.csv',
                              col_names = TRUE,
                              col_types = list(
                                id = col_integer(),
                                value = col_double()))

expression <- read.table('expression.txt', 
                         header=TRUE, 
                         sep = "\t", 
                         row.names=1,
                         quote = "")

training_set_results <- read.table('training_set_answers.txt',
                                   header=TRUE,
                                   sep = '\t',
                                   row.names=1,
                                   quote = "")

expression = as_tibble(rownames_to_column(as.data.frame(t(expression)), var='cellline'))
training_set_results = as_tibble(rownames_to_column(training_set_results, var='cellline'))
```

First, I want to make sure the expression set is normalized properly.  To do that, I evaluate the quantiles for each cell line, since I'm not actually interested in what the actual values, I'm interested in how much they vary.

```{r, validation}
expression_quantiles <- apply(expression[,-1], 1, quantile, probs=c(.25,.5, .75))
apply(expression_quantiles, 1, var)
```

From the results, I can see the quantiles are all similar, and am satisified that the expression data does not require any further normalization.


```{r, variances}
par(mfrow=c(1,2))
variances <- apply(expression[,-1], 2, var)
histogram.data <- hist(variances, breaks=30, plot=TRUE)
histogram.data$counts <- log(histogram.data$counts, 10)
plot(histogram.data, ylim=c(0, 5), ylab=expression("log"[10]*" Frequency"))

```

Normally, I would make the cut-off significantly higher on variance (than 2), the two major computations to do are a covariance matrix for all vectors/features, and then determining the eigen values.  If we decided to calculate the covariance of all 18632 genes, that would be $18623^2$ variance calculations (where each feature has 39 features to it).  Due to the exponential increase, we want to bring the feature size down somewhat, and having something in the neighborhood of ~1000 is perfectly doable for modern computers. 


```{r, pca_1}
features_of_interest <- colnames(expression[,-1])[which(variances > 2)]
expressions_pruned <- expression[,features_of_interest]
```

At this point we have `r length(features_of_interest)` features, and given that we have `r dim(expression)[1]` observations, this feature space is still far too large.  So we can perform a PCA by doing some dimension reduction.


```{r, pca_2}
ev <- eigen(cov(expressions_pruned))
expressions_pca <- as.data.frame(as.matrix(expressions_pruned) %*% ev$vectors[,1:length(ev$values[which(ev$values > .1)])])

# or alternatively
# pca <- prcomp(expressions_pruned, scale=TRUE)
```

We ended up going down to `r length(ev$values[which(ev$values > .1)])` dimensions, and given our sample size that's far more managable.

```{r, train_data_construction}
training_data <- bind_cols(subtypes, expressions_pca)
```

At this point, I am now ready to begin my bagging/bootstrapping.

```{r combining_training_data}
#rosemary messing around
training_data_supervised <- left_join(training_data[1:25,],training_set_results,by = "cellline")
training_data_supervised <- gather(training_data_supervised, "drug", "response", 41:52)

```
So now we have the predictor factors and a column for drug and response each, for easier feeding into prediction models for supervised learning.  So we can use everything to predict "response"

```{r basic modeling}
#lm
linear <- lm(response ~ . - cellline, training_data_supervised)
summary(linear)

#svm
library(e1071)
support <- svm(response ~., training_data_supervised)
summary(support)
support.preditself <- predict(support,training_data_supervised)
table(pred = support.preditself, true = training_data_supervised[1:156,]$response)
```
These aren't models I expected to represent the data well, just making sure the data can run.

The linear model isn't even using most of our factors, and is pretty bad.  Forcing it to use the factors made it use about 9 of the eigenvectors.  Not that we expected it to work particularly well.

The SVM is also not working very well.  I chopped off at 158 because the prediction was mysteriously only 158 chars long, so this is probably wrong somewhere.  It'd be better to be predicting the test data instead of predicting itself in any case, so we should recheck this when we have the test data in.

But, they technically work!