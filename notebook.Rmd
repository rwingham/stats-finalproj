---
title: "Data Import and Rudamentary Analysis"
output: html_notebook
---

```{r, setup}
library(tidyverse)
library(e1071) # for SVMs
library(ipred) # for bagging
library(foreach) # also for bagging
library(randomForest)

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.
```{r, import}
subtypes <- read_tsv('subtypes.txt', 
                     col_names=TRUE,
                     col_types = list(
                       cellline = col_character(),
                       subtype = col_character())
                     )

subtypes <- transform(subtypes, 'normal_like' = as.numeric(subtype == "Normal-like"),
                                'luminal' = as.numeric(subtype == "Luminal"),
                                'claudin_low' = as.numeric(subtype == "Claudin-low"),
                                'basal' = as.numeric(subtype == "Basal"))
subtypes$subtype <- NULL


scoring <- read_csv('scoring_and_test_set_id_mappings.csv', 
                    col_names = TRUE,
                    col_types = list(
                      cellline = col_character(), 
                      drug = col_character(),
                      id = col_integer(),
                      Usage = col_character()))
# discovered drug naming mismatch, making them the same this way
scoring$drug <- gsub("-", ".", scoring$drug)

sample_submission <- read_csv('rand_sub_cont.csv',
                              col_names = TRUE,
                              col_types = list(
                                id = col_integer(),
                                value = col_double()))

expression <- read.table('expression.txt', 
                         header=TRUE, 
                         sep = "\t", 
                         row.names=1,
                         quote = "",
                         check.names=FALSE)


training_set_results <- read.table('training_set_answers.txt',
                                   header=TRUE,
                                   sep = '\t',
                                   row.names=1,
                                   quote = "")


expression = as_tibble(rownames_to_column(as.data.frame(t(expression)), var='cellline'))
training_set_results = as_tibble(rownames_to_column(training_set_results, var='cellline'))
```


First, I want to make sure the expression set is normalized properly.  To do that, I evaluate the quantiles for each cell line, since I'm not actually interested in what the actual values, I'm interested in how much they vary.

```{r, validation}
expression_quantiles <- apply(expression[,-1], 1, quantile, probs=c(.25,.5, .75))
apply(expression_quantiles, 1, var)
```

From the results, I can see the quantiles are all similar, and am satisified that the expression data does not require any further normalization.


```{r, variances}
par(mfrow=c(1,2))
variances <- apply(expression[,-1], 2, var)
histogram.data <- hist(variances, breaks=30, plot=TRUE)
histogram.data$counts <- log(histogram.data$counts, 10)
plot(histogram.data, ylim=c(0, 5), ylab=expression("log"[10]*" Frequency"))

```

Normally, I would make the cut-off significantly higher on variance (than 2), the two major computations to do are a covariance matrix for all vectors/features, and then determining the eigen values.  If we decided to calculate the covariance of all 18632 genes, that would be $18623^2$ variance calculations (where each feature has 39 features to it).  Due to the exponential increase, we want to bring the feature size down somewhat, and having something in the neighborhood of ~1000 is perfectly doable for modern computers. 


```{r, pca_1}
features_of_interest <- colnames(expression[,-1])[which(variances > 2)]
expressions_pruned <- expression[,features_of_interest]
```

At this point we have `r length(features_of_interest)` features, and given that we have `r dim(expression)[1]` observations, this feature space is still far too large.  So we can perform a PCA by doing some dimension reduction.


# Note - Add Plot of Eigen Values (errors)

```{r, pca_2}
ev <- eigen(cov(expressions_pruned))
expressions_pca <- as.data.frame(as.matrix(expressions_pruned) %*% ev$vectors[,1:length(ev$values[which(ev$values > .1)])])

# or alternatively
# pca <- prcomp(expressions_pruned, scale=TRUE)
```

We ended up going down to `r length(ev$values[which(ev$values > .1)])` dimensions, and given our sample size that's far more managable.  At this point, I am now ready to begin my bagging/bootstrapping.

```{r combining_training_data}
#rosemary messing around
# library(broom)
training_data <- inner_join(training_set_results,
                            bind_cols(subtypes, expressions_pca),
                            by = "cellline")

training_data <- gather(training_data, "drug", "response", 2:13)

```


```{r}
test_data <- inner_join(tibble('cellline' = setdiff(expression$cellline, 
                                training_set_results$cellline)),     
                        bind_cols(subtypes, expressions_pca), 
                        by = 'cellline')
```



Now that we have training data in place, we need to develop a different model for every different drug, which we can do using a group-by operation.


```{r, group_by_lm}
# models <- training_data %>% 
#             select(-cellline) %>% 
#             group_by(drug) %>% 
#             nest() %>% 
#             mutate(model = map(data, ~ lm(response ~ ., data = .)))
```

So now we have the predictor factors and a column for drug and response each, for easier feeding into prediction models for supervised learning.  So we can use everything to predict "response"


```{r, svm}
# support <- svm(response ~., training_data)
# summary(support)
# support.preditself <- predict(support,training_data)
# table(pred = support.preditself, true = training_data[1:156,]$response)
```
These aren't models I expected to represent the data well, just making sure the data can run.

The linear model isn't even using most of our factors, and is pretty bad.  Forcing it to use the factors made it use about 9 of the eigenvectors.  Not that we expected it to work particularly well.

The SVM is also not working very well.  I chopped off at 158 because the prediction was mysteriously only 158 chars long, so this is probably wrong somewhere.  It'd be better to be predicting the test data instead of predicting itself in any case, so we should recheck this when we have the test data in.

But, they technically work!



## Dumb Linear Regression (2nd Submission)

Since we need to make two submissions, the second submission will make use of linear regression, to make linear regression work with our obscenely small sample size, we need to trim down the feature space significantly.  From doing a PCA, we will need to trim down to the point that we will be reintroducing good amount of error


```{r dumb_linear_regression}
# expressions_pca <- as.data.frame(as.matrix(expressions_pruned) %*% ev$vectors[,1:length(ev$values[1:24])])
# 
# training_data <- inner_join(training_set_results,
#                             bind_cols(subtypes[1], expressions_pca),
#                             by = "cellline")
# training_data <- gather(training_data, "drug", "response", 2:13)
# 
# predictions <- training_data %>%
#               select(-cellline) %>%
#               group_by(drug) %>%
#               nest() %>%
#               mutate(model = map(data, ~ lm(response ~ ., data = .)))

```




```{r dumb_bagging}
bagging <- function(training, test_vector, drug, iterations=500)
{
  training_data_slim <- filter(training, drug == drug)
  training_data_slim <- subset(training_data_slim, select=-c(drug))
  predictions <- foreach(m=1:iterations, .combine=cbind)  %do% {
    training_bag_set <- sample_n(subset(training_data_slim, 
                                        select=-c(cellline)), 
                                 size=nrow(training_data_slim), 
                                 replace=TRUE)
    suppressWarnings(fit <- lm(response ~ ., data=training_bag_set))
    suppressWarnings(predict(fit, newdata=test_data))
  }
  rowMeans(predictions)
}

```


```{r predict_1}
outcome <- tibble('cellline' = scoring$cellline,
                   'drug' = scoring$drug,
                   'id' = scoring$id,
                   'value' = NA)

for(drug in unique(training_data$drug)){
  predictions <- bagging(training_data, test_data, drug)
  for(j in seq_along(test_data$cellline)){
    cellline <- test_data$cellline[j]
    row <- which(outcome$cellline == cellline & outcome$drug == drug)
    outcome$value[row] <- ifelse(predictions[j] < 0.5, 0, 1)
  }
}

write_csv(outcome[,3:4], 'submission_linear_regression.csv')
```


```{r random_forest}

#a copy with response as a factor for classification algorithms
training_data_class <- training_data
training_data_class$response <- as.factor(training_data_class$response)
training_data_class$drug <- as.factor(training_data_class$drug)
training_data_class$cellline <- as.factor(training_data_class$cellline)

#doesn't work with the test data because of column # diffs :(
#forest <- randomForest(response ~ .,training_data_class, xtest=test_data)
#forest_predictions <- data.frame(forest$predicted)
#forest_predictions <- bind_cols(data.frame(c(1:300)),forest_predictions)
#write_csv(forest_predictions, 'submission_forest.csv')
```


```{r random_forest_2}
models <- training_data %>%
            select(-cellline) %>%
            group_by(drug) %>%
            nest() %>%
            mutate(model = map(data, ~ randomForest(as.factor(response) ~ ., 
                                                    data=., 
                                                    importance=TRUE, 
                                                    ntree=200000)))


outcome <- tibble('cellline' = scoring$cellline,
                   'drug' = scoring$drug,
                   'id' = scoring$id,
                   'value' = NA)

for(i in seq_along(models$drug)){
  drug <- models$drug[i]
  predictions <- as.vector(predict(models$model[[i]], test_data[,-1]))
  for(j in seq_along(test_data$cellline)){
    cellline <- test_data$cellline[j]
    row <- which(outcome$cellline == cellline & outcome$drug == drug)
    outcome$value[row] <- predictions[j]
  }
}

write_csv(outcome[,3:4], 'submission_random_forests.csv')
```
We keep the factor variables coded as numberic here because random forests favor factor variables with more levels, and including them as factors was actually yielding worse performance.  We assume the factors were overshadowing more useful information from the results of our PCA.

